{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7b9607-0347-4d29-b3cc-ecc1574e2073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fetch model from Hugging face hub, tag and store to mlflow UC registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009a459c-bf69-4453-9e84-65a8605e23bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade einops mlflow-skinny timm torch transformers\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d038e2-2cf0-42e2-bf73-aac1e9868e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade accelerate mlflow-skinny torch torchvision transformers\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69110445-7b4b-45f7-9f62-651db235005c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade accelerate mlflow-skinny optree>=0.13.0 torch torchvision transformers\n",
    "\n",
    "\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bce7b9-7a1c-41ed-9cde-f72eb5d0eecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PIP_REQUIREMENTS = (\n",
    "    \"openai vllm>=0.7.2 httpx==0.27.2 \"\n",
    "    # \"transformers==4.46.3 accelerate==1.0.0 \"\n",
    "    # \"git+https://github.com/huggingface/transformers accelerate \"\n",
    "    \"git+https://github.com/huggingface/transformers.git@336dc69d63d56f232a183a3e7f52790429b871ef \"\n",
    "    \"mlflow==2.19.0 \"\n",
    "    \"git+https://github.com/stikkireddy/mlflow-extensions.git@v0.17.0 \"\n",
    "    \"qwen-vl-utils[decord] \"\n",
    "    \"torch \"\n",
    "    \"torchvision \"\n",
    "    \"optree>=0.13.0 \"\n",
    ")\n",
    "%pip install --upgrade accelerate {PIP_REQUIREMENTS}\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e08593-6603-4af1-8ec4-c031947184c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client  = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919e189e-a63d-4cfb-bddb-2edc95428b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"amine_elhelou\" # Change This/Point to an existing catalog\n",
    "SCHEMA = \"ray_gtm_examples\" # Point to an existing schema\n",
    "VOLUME = \"fashion-images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a37d3996-56c8-4e95-9afa-e0a41c9c1c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mini-InternVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed298db-3991-4a59-8bd3-c5806a36e8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, TensorSpec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_video_example_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/data/Apparel/Boys/Images/images_with_product_ids/10054.jpg\"\n",
    "\n",
    "MODEL_NAME = \"qwen2_5_vl-7b\"\n",
    "\n",
    "# Define the input schema\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"text_input\"),  # User query\n",
    "    ColSpec(\"string\", \"video_path\"),  # Path to the input video file (URI)\n",
    "    ColSpec(\"integer\", \"max_pixels\"),  # Max pixel resolution for processing\n",
    "    ColSpec(\"float\", \"fps\")  # Frames per second for processing\n",
    "])\n",
    "\n",
    "# Define the output schema (generated text response)\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"generated_text\")\n",
    "])\n",
    "\n",
    "# Create the model signature\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Log the pipeline\n",
    "with mlflow.start_run(run_name=\"qwen-video-log-pipeline\"):\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=qwen_pipe,\n",
    "        artifact_path=\"qwen_pipeline\",\n",
    "        # input_example=input_video_example_path,\n",
    "        signature=signature,\n",
    "        registered_model_name=f\"{CATALOG}.{SCHEMA}.{QWEN_MODEL_NAME}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be9f9a7-0bed-4949-abdd-ef14f8bb2fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Qwen2.5 VL 7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a393a2ec-181a-4fe5-876d-0a24afd32028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d510d31-84ae-4fa3-b37c-2f3c5785627e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09d9c2a-04ec-46d6-a445-455f390e4e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d456ce6b-8801-4ba8-a5b1-e8ba9b07ca7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build pipeline"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, pipeline\n",
    "# from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float32 #.float16 for model size reduction\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# TO-DO: need to create a custom pipeline https://huggingface.co/docs/transformers/en/add_new_pipeline\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "# model_kwargs = {\n",
    "# \"max_model_len\":10000,\n",
    "# \"max_num_seqs\":5,\n",
    "# \"min_pixels\" : 28 * 28,\n",
    "# \"max_pixels\" : 1280 * 28 * 28,\n",
    "# \"fps\" : 1\n",
    "# }\n",
    "\n",
    "qwen_pipe = pipeline(\n",
    "    \"object-detection\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.image_processor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    # device=device,\n",
    "    # model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e402e0-6af1-46ec-93f0-70f8a777742b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Snap model to UC registry [Recommended]"
    }
   },
   "outputs": [],
   "source": [
    "input_video_example_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/download.mp4\"\n",
    "\n",
    "QWEN_MODEL_NAME = \"qwen2_5_vl-7b\"\n",
    "\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, TensorSpec\n",
    "import numpy as np\n",
    "\n",
    "# Define the input schema\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"string\", \"text_input\"),  # User query\n",
    "    ColSpec(\"string\", \"video_path\"),  # Path to the input video file (URI)\n",
    "    ColSpec(\"integer\", \"max_pixels\"),  # Max pixel resolution for processing\n",
    "    ColSpec(\"float\", \"fps\")  # Frames per second for processing\n",
    "])\n",
    "\n",
    "# Define the output schema (generated text response)\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"string\", \"generated_text\")\n",
    "])\n",
    "\n",
    "# Create the model signature\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Log the pipeline\n",
    "with mlflow.start_run(run_name=\"qwen-video-log-pipeline\"):\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=qwen_pipe,\n",
    "        artifact_path=\"qwen_pipeline\",\n",
    "        # input_example=input_video_example_path,\n",
    "        signature=signature,\n",
    "        registered_model_name=f\"{CATALOG}.{SCHEMA}.{QWEN_MODEL_NAME}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622691ff-dec5-4002-ad7f-4b72086e19c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tag model as @Production"
    }
   },
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(\n",
    "  name=f\"{CATALOG}.{SCHEMA}.{QWEN_MODEL_NAME}\",\n",
    "  version=model_info.registered_model_version,\n",
    "  alias=\"production\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_Download_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
