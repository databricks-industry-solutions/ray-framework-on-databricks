{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0857c2-24f8-4651-a32d-3b414380814c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Automatic-Speech-Recognition batch inference reference solution\n",
    "\n",
    "Tested on:\n",
    "```\n",
    "* MLR 15.4LTS GPU Runtime\n",
    "* Collection of `.wav` files from the LJSpeech dataset\n",
    "* GPU Cluster with 1 driver node and 1 worker node of `g5.12xlarge[A10G]`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb7f487-6f24-4883-bd57-01af27045919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANT:**\n",
    "\n",
    "Set these `spark configs` on the cluster before starting it:\n",
    "\n",
    "* `spark.databricks.pyspark.dataFrameChunk.enabled true`\n",
    "* `spark.task.resource.gpu.amount 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf963c0-cce5-47dc-99f6-81093190ee98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using vllm>=0.7.0 as it supports whisper and manually updating numba due to conflicts\n",
    "%pip install -qU databricks-sdk numba==0.61.0 pydub ray vllm==0.7.0  \n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29cf809-00f8-4564-a4c7-390e16ec6eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e783bbc5-ff99-49c8-8fa5-5b682cf5056f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import pydub\n",
    "import ray\n",
    "import ssl\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "from transformers import pipeline\n",
    "from util import stage_registered_model, flatten_folder\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cd6bd5-84e2-4afb-b6f5-33a12d2aec50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and start Ray cluster\n",
    "Some best practices for scaling up Ray clusters [here](https://docs.databricks.com/en/machine-learning/ray/scale-ray.html#scale-ray-clusters-on-databricks) :\n",
    "* `num_cpus_*` always leave 1 CPU core for spark so value should be <= max cores per worker - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ce314d-5a8b-4513-a963-575d391dd62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_cpu_cores_per_worker = 48-1 # total cpu's present in each worker node (g5.12xlarge)\n",
    "num_cpus_head_node = \t48-1 # total cpu's present in the driver node (g5.12xlarge)\n",
    "num_gpu_per_worker = 4\n",
    "num_gpus_head_node = 4\n",
    "\n",
    "# Set databricks credentials as env vars\n",
    "mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=1,\n",
    "  max_worker_nodes=1,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_gpus_head_node= num_gpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf25452a-0e40-4b2a-a4c4-7ece6747614f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"amine_elhelou\" # system\n",
    "SCHEMA = \"ray_gtm_examples\" # ai\n",
    "ASR_MODEL_NAME = \"whisper-large-v3-turbo\" # whisper-large-v3\n",
    "PII_MODEL_NAME = \"piiranha-v1\"\n",
    "MODEL_ALIAS = \"Production\"\n",
    "# MODEL_VERSION = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9984d5d1-e33c-4826-9091-6761649549b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pre-requisite: download the models from MLflow registry into every node once to avoid multiple download conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e910d81-d381-4aa3-9d4b-733fb3ab4734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from util import run_on_every_node\n",
    "\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758015b9-63b5-41e9-9e2e-c65970f5513a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create remote function which will get model from mlflow directly into local worker disk"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1)\n",
    "def download_model(catalog,\n",
    "                  schema ,\n",
    "                  model_name, \n",
    "                  alias = \"Production\",\n",
    "                  local_base_path = \"/local_disk0/models/\",\n",
    "                  overwrite = False):\n",
    "    model_weights_path = stage_registered_model(\n",
    "                  catalog = CATALOG,\n",
    "                  schema =  SCHEMA,\n",
    "                  model_name = model_name,\n",
    "                  alias = alias,\n",
    "                  local_base_path = local_base_path,\n",
    "                  overwrite = overwrite)\n",
    "    flatten_folder(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6e4d55-ac6e-4964-ae40-f3350085c5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# Point to UC registry (in case not default)\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Execute\n",
    "_ = run_on_every_node(download_model , **{\n",
    "                  \"catalog\": CATALOG,\n",
    "                  \"schema\": SCHEMA,\n",
    "                  \"model_name\": ASR_MODEL_NAME,\n",
    "                  \"alias\": MODEL_ALIAS\n",
    "                  })\n",
    "_ = run_on_every_node(download_model , **{\n",
    "                  \"catalog\": CATALOG,\n",
    "                  \"schema\": SCHEMA,\n",
    "                  \"model_name\": PII_MODEL_NAME,\n",
    "                  \"alias\": MODEL_ALIAS\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae72a963-389d-4131-9746-397a0b14fda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write ray-friendly `__call__`-able classes for batch processing\n",
    "\n",
    "For VLLM, one parameter to configure would be:\n",
    "\n",
    "* `gpu_memory_utilization`: will define how many model instances will be created in a single GPU. This would depend on model's size and GPU VRAM in theory. For example: `whisper-v3-large` is ~1.55B at FP32 would require 10GB of memory and an A10G's VRAM is 24GB implies that this parameter could be set to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d16f80-dae3-4a4f-adde-2cb6dd28bdeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingest and transcribe pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872ddda8-0081-4ed5-8dcb-c1e93352d97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConverttoPrompt:\n",
    "    \"\"\"\n",
    "    Class which whill read audio files and convert them to numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, audio_filenames):\n",
    "        # CHANGE THIS BASED ON YOUR AUDIO FILE SOURCES\n",
    "        a = pydub.AudioSegment.from_wav(audio_filenames) # .from_mp3()audio_filenames\n",
    "        y = np.array(a.get_array_of_samples())\n",
    "        if a.channels == 2:\n",
    "            y = y.reshape((-1, 2))\n",
    "\n",
    "        array = np.float32(y) / 2**15\n",
    "        frame_rate =  a.frame_rate\n",
    "        return array,frame_rate\n",
    "\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        array ,frame_rate = self.transform(row[\"file_path\"])\n",
    "        \n",
    "        row['array'] = list(array)\n",
    "        row['frame_rate'] = frame_rate\n",
    "        return row\n",
    "\n",
    "\n",
    "class WhisperTranscription:\n",
    "    \"\"\"\n",
    "    Class which will handle transcription of audio files (in batch fashion using VLLM)\n",
    "    \"\"\"\n",
    "    def __init__(self, catalog:str, schema:str, model_name:str, model_alias:str = \"Production\"):\n",
    "        self.unverified_context = ssl._create_unverified_context()\n",
    "        print(\"Loading model from UC registry...\")\n",
    "        model_weights_path = stage_registered_model(\n",
    "                            catalog = catalog, # \"system\"\n",
    "                            schema = schema, #\"ai\"\n",
    "                            model_name = model_name, # whisper_large_v3\",\n",
    "                            alias = model_alias,\n",
    "                            # version = model_version,\n",
    "                            local_base_path = \"/local_disk0/models/\",\n",
    "                            overwrite = False)\n",
    "        flatten_folder(model_weights_path)\n",
    "        model_weights_path = str(model_weights_path)  #convert from Posit to string required by TF\n",
    "        self.WHISPER_MODEL_PATH = model_weights_path\n",
    "\n",
    "        # Create VLLM pipeline object\n",
    "        self.transcription_pipeline = LLM(\n",
    "                            model=model_weights_path,\n",
    "                            max_model_len=448, # Max chunk size to be sliced into for long audio transcripts (READ VLLM config for whisper-v3-large-turbo model)\n",
    "                            max_num_seqs=400,\n",
    "                            kv_cache_dtype=\"fp8\",\n",
    "                            enforce_eager=True,\n",
    "                            gpu_memory_utilization = 1) # How many models to load per GPU, depending on model size and GPU RAM (default to 1 to avoid OOM errors)\n",
    "        print(\"Model loaded...\")\n",
    "\n",
    "    def transform(self, row):\n",
    "        \"\"\"\n",
    "        Format the input audio stream/array to be passed to the VLLM pipeline according to how the model is expecting it (e.g. {\n",
    "            \"prompt\" : \"<|startoftranscript|>\",\n",
    "            \"multi_modal_data\": \n",
    "                { \"audio\" : (<array, frame_rate>) }\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare batch of prompts to be passed to the VLLM pipeline\n",
    "        prompts = []\n",
    "        for array,frame_rate in zip(list(row['array']),list(row['frame_rate'])):\n",
    "            prompts.append({\"prompt\": \"<|startoftranscript|>\",\n",
    "                                \"multi_modal_data\":{\"audio\": (array,frame_rate)}})\n",
    "            \n",
    "        return prompts\n",
    "\n",
    "\n",
    "    def __call__(self, row) -> str:\n",
    "        \"\"\"\n",
    "        Call method applying all pipeline steps (in batch)\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a sampling params inference object\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        prompts = self.transform(row)\n",
    "\n",
    "        outputs = self.transcription_pipeline.generate(prompts, sampling_params)\n",
    "\n",
    "        del row['array']\n",
    "        del row['frame_rate']\n",
    "\n",
    "        row['transcription'] = [ output.outputs[0].text for output in outputs ]\n",
    "\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460f9967-a2a5-4c3b-80e6-56c91ba296c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PII Redaction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df31c6d6-fc81-4840-811f-6878d7805c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "class PIIRedaction:\n",
    "    \"\"\"\n",
    "    Class which will handle redaction of audio transcripts (per record)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, catalog:str, schema:str, model_name:str, model_alias:str = \"Production\"):        \n",
    "        print(\"Loading PII-redaction model from UC registry...\")\n",
    "        model_weights_path = stage_registered_model(\n",
    "                            catalog = catalog,\n",
    "                            schema = schema,\n",
    "                            model_name = model_name,\n",
    "                            alias = model_alias,\n",
    "                            # version = model_version,\n",
    "                            local_base_path = \"/local_disk0/models/\",\n",
    "                            overwrite = False)\n",
    "        flatten_folder(model_weights_path)\n",
    "        model_weights_path = str(model_weights_path)  #convert from Posit to string required by TF\n",
    "        self.PII_MODEL_PATH = model_weights_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_weights_path) # model_id\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_weights_path) # model_id\n",
    "\n",
    "    def _mask_pii(self, text, aggregate_redaction=False):\n",
    "        \"\"\"\n",
    "        Apply redaction to the text based on source code provided by the PII/NER Model\n",
    "        \"\"\"\n",
    "\n",
    "        def apply_redaction(masked_text, start, end, pii_type, aggregate_redaction):\n",
    "            for j in range(start, end):\n",
    "                masked_text[j] = ''\n",
    "            if aggregate_redaction:\n",
    "                masked_text[start] = '[redacted]'\n",
    "            else:\n",
    "                masked_text[start] = f'[{pii_type}]'\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get the model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Convert token predictions to word predictions\n",
    "        encoded_inputs = self.tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        offset_mapping = encoded_inputs['offset_mapping']\n",
    "\n",
    "        masked_text = list(text)\n",
    "        is_redacting = False\n",
    "        redaction_start = 0\n",
    "        current_pii_type = ''\n",
    "\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            if start == end:  # Special token\n",
    "                continue\n",
    "\n",
    "            label = predictions[0][i].item()\n",
    "            if label != self.model.config.label2id['O']:  # Non-O label\n",
    "                pii_type = self.model.config.id2label[label]\n",
    "                if not is_redacting:\n",
    "                    is_redacting = True\n",
    "                    redaction_start = start\n",
    "                    current_pii_type = pii_type\n",
    "                elif not aggregate_redaction and pii_type != current_pii_type:\n",
    "                    # End current redaction and start a new one\n",
    "                    apply_redaction(masked_text, redaction_start, start, current_pii_type, aggregate_redaction)\n",
    "                    redaction_start = start\n",
    "                    current_pii_type = pii_type\n",
    "            else:\n",
    "                if is_redacting:\n",
    "                    apply_redaction(masked_text, redaction_start, end, current_pii_type, aggregate_redaction)\n",
    "                    is_redacting = False\n",
    "\n",
    "        # Handle case where PII is at the end of the text\n",
    "        if is_redacting:\n",
    "            apply_redaction(masked_text, redaction_start, len(masked_text), current_pii_type, aggregate_redaction)\n",
    "\n",
    "        return ''.join(masked_text)\n",
    "    \n",
    "    def __call__(self, row:dict) -> dict:\n",
    "        row[\"redacted_text\"] = self._mask_pii(row[\"transcription\"], aggregate_redaction=False)\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363f6980-8862-499c-9592-3f2529ef37db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare batch job\n",
    "\n",
    "1. Point to input Delta table containing file paths\n",
    "2. Select UC model names and `@alias` _(or version)_\n",
    "3. Write ray inference code\n",
    "4. Apply batch job and write/materialize outputs to Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31f4b60-73b3-4e5c-90d8-a0c427087d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Read input Delta table containing audio file's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a5686f-7159-4984-b1a6-5ba466cefa35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLENAME = f\"{CATALOG}.{SCHEMA}.recordings_file_reference\" #CATALOG.SCHEMA.table\n",
    "audio_files_reference_df = spark.table(TABLENAME)\n",
    "# audio_files_reference_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbaa10e7-6469-4b3b-9fcf-af689ca923c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Write ray batch pipeline\n",
    "\n",
    "using [`map`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map) and [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html)\n",
    "\n",
    "Some relevant parameters to define are:\n",
    "* `num_cpus`: for CPU intensive workloads (i.e. read the audio files) - defines how many ray-cores to use for individual tasks (default is `1 Ray-Core/CPU == 1 CPU Physical Core`). It can be defined as a fraction to oversubscribe a single physical core with multiple tasks\n",
    "\n",
    "* `num_gpus`: for GPU intensive workloads - defines how many (fractionnal) GPU(s) a single task/batch will use\n",
    "\n",
    "* `concurrency`: how many parallel tasks to run in parallel `Tuple(min,max)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b23821a-9a87-4cef-a4d2-30b9d58ea1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IF USING `whisper-large-v3` directly from UC's `system.ai` then set `CATALOG = system` and `SCHEMA = ai`** _(and `model_version = 1`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ffb6f9-82a3-4383-bfbe-e2e2c124572b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = ray.data.from_spark(audio_files_reference_df)\n",
    "ds = ds.repartition(200)\\\n",
    "        .map(\n",
    "            ConverttoPrompt,\n",
    "            concurrency=(40,94), # Can go up to total sum of cores\n",
    "            num_cpus=1,\n",
    "        )\\\n",
    "        .map_batches(\n",
    "              WhisperTranscription,\n",
    "              fn_constructor_kwargs={\n",
    "                  \"catalog\": CATALOG,\n",
    "                  \"schema\": SCHEMA,\n",
    "                  \"model_name\": ASR_MODEL_NAME,\n",
    "                  \"model_alias\": MODEL_ALIAS\n",
    "                  },\n",
    "              concurrency=(6,12), # Up to max number of GPUs\n",
    "              num_gpus=.6, # Individual batches will utilize  up to 60% of GPU's memory <==> 2 batches in parallel per GPU\n",
    "              batch_size = 128\n",
    "          )\\\n",
    "          .map(\n",
    "              PIIRedaction,\n",
    "              fn_constructor_kwargs={\n",
    "                  \"catalog\": CATALOG,\n",
    "                  \"schema\": SCHEMA,\n",
    "                  \"model_name\": PII_MODEL_NAME,\n",
    "                  \"model_alias\": MODEL_ALIAS\n",
    "                  },\n",
    "              concurrency=(10,24),\n",
    "              num_gpus=float(.2) # One task/record will utilize up to 20% of GPU's memory <==> 5 tasks in parallel per GPU\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90247e58-5ea5-4660-b17b-69e17b7d2787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Temporary directory for ray-uc-volumes-fuse (to write to Delta natively)\n",
    "VOLUME = \"temp\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "\n",
    "tmp_dir_fs = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/tempDoc\"\n",
    "dbutils.fs.mkdirs(tmp_dir_fs)\n",
    "os.environ[\"RAY_UC_VOLUMES_FUSE_TEMP_DIR\"] = tmp_dir_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38764113-1bfa-408f-8aff-c27760193353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds.write_databricks_table(\n",
    "  f\"{CATALOG}.{SCHEMA}.whisper_transcriptions_redacted_silver_piiranha_v2\",\n",
    "  mode = \"overwrite\", #append/merge\n",
    "  mergeSchema = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3cbcd8e-d80f-4b54-a8a5-07701e065b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(f\"{CATALOG}.{SCHEMA}.whisper_transcriptions_redacted_silver_piiranha_v2\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02 Transcribe with Whisper on Ray VLLM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
