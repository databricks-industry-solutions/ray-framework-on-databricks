{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f54af6-8217-4404-afcb-abc5c9b90447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parallel demand forecasting at scale using Ray Tune and Ray Data\n",
    "\n",
    "Batch training and tuning are common tasks in machine learning use-cases. They require training simple models, on data batches, typcially corresponding to different locations, products, etc. Batch training can take less time to process all the data at once, but only if those batches can run in parallel!\n",
    "\n",
    "This notebook showcases how to conduct batch forecasting with NeuralProphet. NeuralProphet is a popular open-source library developed by Facebook and designed for automatic forecasting of univariate time series data. \n",
    "<br></br>\n",
    "<div style=\"text-align: center; line-height: 5; padding-top: 20px;  padding-bottom: 20px;\">\n",
    "  <img src=\"https://docs.ray.io/en/master/_images/batch-training.svg\" alt='Push compute' height=\"300\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "For the data, we will use the M5 walmart dataset.This popular tabular dataset contains historical sales of products for different locations and regions in USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f0e53d-e64f-4c9b-bd6c-4bf8c34594b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## This notebook has been tested with ML DBR 16.4 with the below cluster config\n",
    "**Head node** : 28GB 4 Cores <br>\n",
    "**Worker nodes** : 64GB 16 Cores  <br>\n",
    "**max workers** : 3  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea87e6ab-81a7-4e53-aa37-e9e556165fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manage Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be647153-2f42-42a1-b071-8c72368d8875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install neuralprophet ray[default,tune]==2.41.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed29296-1dd4-4590-8387-254d0b9ef18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Base Python imports\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "from datetime import date, datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# External libraries\n",
    "import mlflow\n",
    "import neuralprophet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import ray\n",
    "import requests\n",
    "import torch\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from neuralprophet import NeuralProphet\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col\n",
    "from ray import train, tune\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.tune.search.concurrency_limiter import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e4e2f2-e2ea-44eb-9029-e1ed66f27571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = mlflow.models.ModelConfig(development_config='config.yaml')\n",
    "CATALOG = config.get('catalog')\n",
    "SCHEMA = config.get('schema')\n",
    "VOLUME = config.get('volume')\n",
    "\n",
    "# Assumes 01_data has been run\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4adeed54-8bd3-4631-bdb1-12090dc478a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fed952f-7d2e-4bff-bf73-9a2b60325b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get cluster info\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "host_name = ctx.tags().get(\"browserHostName\").get()\n",
    "host_token = ctx.apiToken().get()\n",
    "cluster_id = ctx.tags().get(\"clusterId\").get()\n",
    "response = requests.get(\n",
    "    f'https://{host_name}/api/2.1/clusters/get?cluster_id={cluster_id}',\n",
    "    headers={'Authorization': f'Bearer {host_token}'}\n",
    "  ).json()\n",
    "\n",
    "# Get autoscale stats\n",
    "if \"autoscale\" in response:\n",
    "  min_workers = response['autoscale'][\"min_workers\"]\n",
    "  max_workers = response['autoscale'][\"max_workers\"]\n",
    "else:\n",
    "  min_workers = 1\n",
    "  max_workers = 1\n",
    "\n",
    "# Get CPU information\n",
    "current_workers = len(response.get('executors',[]))\n",
    "driver_cpus = multiprocessing.cpu_count()\n",
    "worker_cpus = response.get('cluster_cores') - driver_cpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97425613-2df2-4e91-81aa-62df0510c847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25071795-cc69-45e3-ac02-1d725f943729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if config.get('restart_ray_each_run'):\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# We need to pass these to the environment in BEFORE caling setup_ray_cluster so that MLFlow works \n",
    "mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']\n",
    "\n",
    "# We keep two nodes from each worker and half the driver for spark\n",
    "ray_conf = setup_ray_cluster(\n",
    "  min_worker_nodes=min_workers,\n",
    "  max_worker_nodes=max_workers,\n",
    "  num_cpus_head_node=int(driver_cpus/2),\n",
    "  num_cpus_per_node=int(worker_cpus-2),\n",
    "  num_gpus_head_node=0,\n",
    "  num_gpus_worker_node=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bceb40f-05be-495a-899d-bcbdc9ffa390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distribute dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0442ed55-de85-405e-8789-9704c4e84b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_walmart = spark.read.table(config.get(\"final_table\"))\n",
    "\n",
    "window_spec = Window.orderBy(\n",
    "    \"state_id\", \"store_id\", \"cat_id\", \"dept_id\", \"item_id\"\n",
    ")\n",
    "\n",
    "sdf_walmart_with_model_num = (\n",
    "    sdf_walmart.withColumn(\n",
    "        \"item_num\", F.dense_rank().over(window_spec)\n",
    "    )\n",
    "    .filter(F.col(\"item_num\") <= config.get(\"num_items\"))\n",
    "    .withColumn(\n",
    "        \"model_num\",\n",
    "        F.ceil(F.col(\"item_num\") / config.get(\"items_per_model\")),\n",
    "    )\n",
    "    .withColumn(\"y\", F.col(\"sell_price\") * F.col(\"sale_quantity\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(sdf_walmart.count())\n",
    "display(sdf_walmart_with_model_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144134b2-0da3-449b-bc10-1d5258757345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## There are multiple ways to convert data from the lakehouse to Ray Data , refer to the [documentation](https://docs.databricks.com/en/machine-learning/ray/connect-spark-ray.html) for more details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d121b08-752e-4865-9b85-40b1daa7cc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In Databricks Runtime, ray.data.from_spark() requires the DataFrame chunk API to efficiently transfer data from Spark to Ray. This configuration enables chunked reading of Spark DataFrames, which is more memory-efficient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed8e6ef-0cb0-47a5-aeb2-9c7d290ff68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the temporary directory for Ray to use Unity Catalog Volumes\n",
    "os.environ['RAY_UC_VOLUMES_FUSE_TEMP_DIR'] = f'/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/ray_temp'\n",
    "\n",
    "# Convert spark Dataframe to Ray \n",
    "sdf_ray = ray.data.from_spark(\n",
    "    sdf_walmart_with_model_num, \n",
    "    use_spark_chunk_api=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81eaec72-129a-43f4-a22b-b34791bd0c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Many model Forecasting with Ray Tune and Ray Data\n",
    "Ray Tune is a powerful library for hyperparameter tuning, designed to simplify the development of distributed applications. It allows you to efficiently sample hyperparameters and get optimized results on your objective function. Ray Tune provides a variety of state-of-the-art hyperparameter tuning algorithms for optimizing model performance. \n",
    "\n",
    "To use Ray Tune for hyperparameter tuning, you can follow these steps:\n",
    "- Define your training function and objective function.\n",
    "- Specify the hyperparameters and their search space.\n",
    "- Define the pyspark udf function which runs ray tune for each Hierarchial model for the chosen search algorithm and scheduler.\n",
    "- Run the pyspark job and get the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fe1b62-29c5-4674-abe8-479f726e733a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1 : Define the training and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9f497d-68e4-4dcc-a883-5f4df2d4f2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ray_trial(config, df, cpu_resources_per_trial):\n",
    "  \"\"\"\n",
    "  Single ray trial of parameter config \n",
    "  This runs a NeuralProphet model based on the given config and then loads  \n",
    "  \"\"\"\n",
    "\n",
    "  torch.set_num_threads(int(cpu_resources_per_trial)) # Pass the correct cpu to use to improve multi-threading\n",
    "  test_cutoff = df['ds'].max() - pd.Timedelta(days=7) # Take 7 day test cut-ogg\n",
    "  df_train = df[df['ds'] < test_cutoff]\n",
    "  df_test = df\n",
    "  trainer_config = {}\n",
    "\n",
    "  # Hardcode other Neural prophet parameters\n",
    "  config['n_changepoints'] = 10\n",
    "  config['n_lags'] = 3\n",
    "  config['drop_missing'] = True \n",
    "  config['impute_rolling'] = 1000\n",
    "  config['batch_size'] = 128\n",
    "  config['epochs'] = 10\n",
    "\n",
    "  # Define the Model (it can be any model in our case we use NeuralProphet)\n",
    "  model = NeuralProphet(\n",
    "      accelerator='auto',\n",
    "      trainer_config=trainer_config,\n",
    "      **config\n",
    "  )\n",
    "  start = timeit.default_timer()\n",
    "  \n",
    "  # Train model\n",
    "  progress = model.fit(\n",
    "      df=df_train,\n",
    "      checkpointing =True,\n",
    "      freq=\"D\",\n",
    "      metrics=['RMSE'],\n",
    "      progress='bar'\n",
    "    )\n",
    "  total_time = timeit.default_timer()-start\n",
    "  if np.isnan(progress['RMSE'][0]):\n",
    "    progress.fillna(1000, inplace = True)\n",
    "\n",
    "  d_p = progress.loc[progress['RMSE'] == progress['RMSE'].min()].to_dict(orient='records')\n",
    "  print(\"Loss :\",d_p[0]['Loss'])\n",
    "\n",
    "  # Validate the model and get the RMSE Score\n",
    "  forecast_week = model.predict(df[df['ds'] >= (df['ds'].max() - pd.Timedelta(days=360))])\n",
    "  forecast_week = forecast_week[forecast_week['ds'] >= test_cutoff]\n",
    "  forecast_week.y.fillna(0, inplace=True)\n",
    "  forecast_week.yhat1.fillna(0, inplace=True)\n",
    "  test_rmse = mean_squared_error(forecast_week.yhat1.tolist(), forecast_week.y.tolist(), squared=False)\n",
    "  if np.isnan(test_rmse):\n",
    "    test_rmse = 1000\n",
    "  print(\"test_rmse:\",test_rmse)\n",
    "  \n",
    "  #Push the final metric to track\n",
    "  ray.train.report({\"RMSE\":test_rmse, \"Loss\" :d_p[0]['Loss']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d3ee0d-8432-4926-96e5-c5524cde13e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2 : Define the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212f781f-a89a-4c1c-ad6b-9066b2b65dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "space_str = \"\"\"\n",
    "{\n",
    "  \"learning_rate\": tune.uniform(0.001, 0.1),\n",
    "  \"n_changepoints\": 10,\n",
    "  \"n_lags\": 3, \n",
    "  'drop_missing': True,\n",
    "  'impute_rolling': 1000,\n",
    "  'newer_samples_weight': tune.uniform(1, 7),\n",
    "  'batch_size': 128,\n",
    "  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n",
    "  'epochs': 10\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0beb04b0-c6a6-44bb-b7dd-262d32cf8bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3 : Define the Ray Map Groups udf function which runs ray tune for each Hierarchial model for the chosen search algorithm and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900ef278-5819-408a-a0be-fec158c086a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_trials = config.get('max_concurrent_trials')\n",
    "num_batches = int(config.get('num_items') / config.get('items_per_model'))\n",
    "num_models = sdf_walmart_with_model_num.select(F.max('model_num')).collect()[0][0]\n",
    "total_concurrent_trials = num_models * max_trials\n",
    "print(f\"Total concurrent Trials: {total_concurrent_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270ed3c2-447c-42d0-b8fd-3db3bc632f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def udf_parallel_tune(\n",
    "    df,\n",
    "    experiment_id,\n",
    "    parent_id=None,\n",
    "    cpu_resources_per_trial=2,\n",
    "    gpu_resources_per_trial=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single ray trial of parameter config \n",
    "    This runs a NeuralProphet model based on the given config and then loads  \n",
    "    \"\"\"\n",
    "\n",
    "    def define_by_run_func(trial) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Define-by-run function to create the search space.\n",
    "        For more information, see https://optuna.readthedocs.io/en/stable\\\n",
    "        /tutorial/10_key_features/002_configurations.html\n",
    "        \"\"\"\n",
    "        trial.suggest_int(\"n_estimators\", 10, 200, log=True)\n",
    "        trial.suggest_float(\"newer_samples_weight\", 1, 7)\n",
    "        trial.suggest_categorical(\n",
    "            \"ar_layers\", [[64, 64, 64], [128, 128, 128], [256, 256, 256]]\n",
    "        )\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    model_num = df[\"model_num\"][0]\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"], format=\"%Y-%m-%d\")\n",
    "    df = df.sort_values(by=\"date_time\", ascending=True)\n",
    "    df = df.rename(columns={\"date_time\": \"ds\", \"item_num\": \"ID\"})\n",
    "    df = df[[\"ID\", \"ds\", \"y\"]]\n",
    "\n",
    "    # Use a string to serialize the parameters easier \n",
    "    space = eval(space_str)\n",
    "\n",
    "    tune_resources = (\n",
    "        {\"CPU\": cpu_resources_per_trial}\n",
    "        if gpu_resources_per_trial == 0\n",
    "        else {\"CPU\": cpu_resources_per_trial, \"GPU\": gpu_resources_per_trial}\n",
    "    )\n",
    "\n",
    "    # Define Optuna search algo\n",
    "    searcher = OptunaSearch(space=space, metric=\"RMSE\", mode=\"min\")\n",
    "\n",
    "    # Tune with callback\n",
    "    tuner = tune.Tuner(\n",
    "        ray.tune.with_resources(\n",
    "            ray.tune.with_parameters(\n",
    "                ray_trial, df=df, cpu_resources_per_trial=cpu_resources_per_trial\n",
    "            ),\n",
    "            tune.PlacementGroupFactory([tune_resources]),\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=searcher,\n",
    "            max_concurrent_trials=MAX_TRIALS,\n",
    "            num_samples=MAX_TRIALS * NUM_BATCHES,\n",
    "            reuse_actors=True,  # Highly recommended for short training jobs (NOT RECOMMENDED FOR GPU AND LONG TRAINING JOBS)\n",
    "        ),\n",
    "    )\n",
    "    multinode_results = tuner.fit()\n",
    "    best_trial = multinode_results.get_best_result(\n",
    "        metric=\"RMSE\", mode=\"min\", scope=\"last\"\n",
    "    )\n",
    "    with mlflow.start_run(\n",
    "        run_name=f\"model_{str(model_num)}\",\n",
    "        experiment_id=experiment_id,\n",
    "        tags={\"mlflow.parentRunId\": parent_id.info.run_id},\n",
    "        description=\"run inside ray Map Batches\",\n",
    "    ) as child_run:\n",
    "        for key, value in best_trial.config.items():\n",
    "            mlflow.log_param(key=key, value=str(value))\n",
    "        mlflow.log_metric(key=\"rmse\", value=best_trial.metrics[\"RMSE\"])\n",
    "        mlflow.log_metric(key=\"Loss\", value=best_trial.metrics[\"Loss\"])\n",
    "        # mlflow.pyfunc.log_model(best_trial.last_result['checkpoint'], \"model\")\n",
    "\n",
    "    best_rmse = best_trial.metrics[\"RMSE\"]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model_num\": model_num,\n",
    "                \"model_HPT_time\": str(timeit.default_timer() - start),\n",
    "                \"num_datapoints\": df[\"y\"].count(),\n",
    "                \"RMSE\": best_rmse,\n",
    "                \"space\": str(best_trial.config),\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52e639a-4bec-4218-961b-0a36b325aad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4 : Run the Map_groups wrapped in MLFLOW and get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394138ef-8f3a-4129-8636-936fceaa1001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = config.get('experiment_name')\n",
    "\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "else:\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=\"ray_tune_native_mlflow_callback\", experiment_id=experiment_id\n",
    ") as parent_run:\n",
    "    result_df = (\n",
    "        sdf_ray.groupby(\"model_num\")\n",
    "        .map_groups(\n",
    "            udf_parallel_tune,\n",
    "            concurrency=num_models,\n",
    "            fn_kwargs={\n",
    "                \"parent_id\": parent_run,\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"cpu_resources_per_trial\": 2,\n",
    "                \"gpu_resources_per_trial\": 0,\n",
    "            },\n",
    "            batch_format=\"pandas\",\n",
    "        )\n",
    "        .to_pandas()\n",
    "    )\n",
    "    \n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_ray_tune",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
